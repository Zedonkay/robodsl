// Comprehensive RoboDSL Example - Demonstrating All Features
// This example shows all the capabilities of the RoboDSL language

// Include statements
include "common_config.robodsl"
include <std_msgs/String>

// ONNX Model definition for AI inference
onnx_model object_detection_model {
    config {
        input: input_tensor -> images
        output: output_tensor -> detections
        device: "cuda"
        optimization: "tensorrt"
    }
}

// CUDA kernels block for GPU acceleration
cuda_kernels {
    kernel image_processing_kernel {
        block_size: (16, 16, 1)
        grid_size: (width / 16, height / 16, 1)
        shared_memory: 1024
        use_thrust: true
        input: float* input_image, int width, int height
        output: float* processed_image
        code: {
            __global__ void process_image(float* input, float* output, int w, int h) {
                int x = blockIdx.x * blockDim.x + threadIdx.x;
                int y = blockIdx.y * blockDim.y + threadIdx.y;
                
                if (x < w && y < h) {
                    int idx = y * w + x;
                    // Apply some image processing
                    output[idx] = input[idx] * 1.5f + 0.1f;
                }
            }
        }
    }
    
    kernel matrix_multiply_kernel {
        block_size: (32, 32, 1)
        grid_size: (matrix_size / 32, matrix_size / 32, 1)
        shared_memory: 2048
        use_thrust: false
        input: float* matrix_a, float* matrix_b, int matrix_size
        output: float* result_matrix
        code: {
            __global__ void matrix_multiply(float* A, float* B, float* C, int N) {
                __shared__ float shared_A[32][32];
                __shared__ float shared_B[32][32];
                
                int row = blockIdx.y * blockDim.y + threadIdx.y;
                int col = blockIdx.x * blockDim.x + threadIdx.x;
                
                float sum = 0.0f;
                for (int k = 0; k < N; k += 32) {
                    if (row < N && k + threadIdx.x < N)
                        shared_A[threadIdx.y][threadIdx.x] = A[row * N + k + threadIdx.x];
                    else
                        shared_A[threadIdx.y][threadIdx.x] = 0.0f;
                        
                    if (k + threadIdx.y < N && col < N)
                        shared_B[threadIdx.y][threadIdx.x] = B[(k + threadIdx.y) * N + col];
                    else
                        shared_B[threadIdx.y][threadIdx.x] = 0.0f;
                        
                    __syncthreads();
                    
                    for (int i = 0; i < 32; i++) {
                        sum += shared_A[threadIdx.y][i] * shared_B[i][threadIdx.x];
                    }
                    __syncthreads();
                }
                
                if (row < N && col < N) {
                    C[row * N + col] = sum;
                }
            }
        }
    }
}

// Main processing node with all features
node comprehensive_processor {
    // Parameters with various types
    parameter int max_queue_size = 10
    parameter float processing_rate = 30.0
    parameter string model_path = "/opt/models/detection.onnx"
    parameter bool enable_gpu = true
    parameter int[3] image_size = [640, 480, 3]
    parameter float[4] calibration_matrix = [1.0, 0.0, 0.0, 1.0]
    parameter bool debug_mode = true
    parameter string log_level = "info"
    parameter float timeout = 5.0
    
    // Lifecycle configuration
    lifecycle {
        configure: true
        activate: true
        deactivate: true
        cleanup: true
        shutdown: true
    }
    
    // Timers with expressions
    timer processing_timer: 1000 / processing_rate {
        oneshot: false
        autostart: true
    }
    
    timer calibration_timer: -5000 + 2000 {
        oneshot: true
        autostart: false
    }
    
    timer adaptive_timer: max_queue_size * 100 {
        oneshot: false
        autostart: true
    }
    
    // Topic remaps
    remap from:/camera/image_raw to:/processor/input_image
    remap /cmd_vel:/robot/velocity
    
    // Namespace
    namespace: /robot1
    
    // ROS Publishers with QoS
    publisher /processor/processed_image: "sensor_msgs/Image" {
        qos {
            reliability: 1
            durability: 1
            history: 1
            depth: 10
        }
        queue_size: max_queue_size
    }
    
    publisher /processor/detection_results: "vision_msgs/Detection2DArray" {
        qos {
            reliability: 2
            durability: 2
            history: 2
            depth: 5
        }
    }
    
    // ROS Subscribers with QoS
    subscriber /camera/image_raw: "sensor_msgs/Image" {
        qos {
            reliability: 1
            durability: 1
            history: 1
            depth: 5
        }
        queue_size: 5
    }
    
    subscriber /robot/status: "std_msgs/String"
    
    // ROS Services
    service /processor/reconfigure: "std_srvs/SetBool" {
        qos {
            reliability: 1
            durability: 1
        }
    }
    
    // ROS Service Clients
    client /robot/emergency_stop: "std_srvs/Trigger" {
        qos {
            reliability: 2
            durability: 2
        }
    }
    
    // ROS Actions
    action /processor/calibration: "actionlib/TestAction" {
        qos {
            reliability: 1
            durability: 1
        }
    }
    
    // Flags
    flag debug_mode: true
    flag enable_cuda: true
    flag verbose_logging: false
    
    // C++ method with complex parameters
    method process_image {
        input: float* input_image(input_param_size)
        input: float* calibration_data(4)
        output: float* processed_image(output_param_size)
        output: float* detections
        code: {
            // Image processing implementation
            // Simplified implementation for demonstration
            for (int i = 0; i < input_param_size; i++) {
                processed_image[i] = input_image[i] * 1.5f + 0.1f;
            }
            
            // Run object detection
            if (enable_cuda) {
                // CUDA detection logic
                detections[0] = 1.0f;
            } else {
                // CPU detection logic
                detections[0] = 0.5f;
            }
        }
    }
    
    // Method with dotted names and expressions
    method advanced_processing {
        input: float* image_batch(image_batch_size)
        input: float* config
        output: float* results
        code: {
            for (int i = 0; i < image_batch_size; i++) {
                // Advanced processing logic
                results[i] = image_batch[i] * config[0];
            }
        }
    }
    
    // ONNX model reference
    onnx_model object_detection_model {
        config {
            input: "input_tensor" -> "images"
            output: "output_tensor" -> "detections"
            device: "cuda"
            optimization: "tensorrt"
        }
    }
    
    // Embedded CUDA kernels
    cuda_kernels {
        kernel fast_filter_kernel {
            block_size: (16, 16, 1)
            grid_size: (width / 16, height / 16, 1)
            shared_memory: 512
            use_thrust: true
            input: float* input_data, int width, int height
            output: float* filtered_data
            code: {
                __global__ void fast_filter(float* input, float* output, int w, int h) {
                    int x = blockIdx.x * blockDim.x + threadIdx.x;
                    int y = blockIdx.y * blockDim.y + threadIdx.y;
                    
                    if (x < w && y < h) {
                        int idx = y * w + x;
                        // Simple filter operation
                        output[idx] = input[idx] * 0.8f;
                    }
                }
            }
        }
    }
}

// Pipeline definition for multi-stage processing
pipeline vision_pipeline {
    stage image_preprocessing {
        input: "/camera/image_raw"
        output: "/pipeline/preprocessed"
        method: "preprocess_image"
        topic: /pipeline/preprocessing_status
    }
    
    stage object_detection {
        input: "/pipeline/preprocessed"
        output: "/pipeline/detections"
        method: "detect_objects"
        model: "object_detection_model"
        cuda_kernel: "image_processing_kernel"
        onnx_model: "object_detection_model"
        topic: /pipeline/detection_status
    }
    
    stage post_processing {
        input: "/pipeline/detections"
        output: "/pipeline/final_results"
        method: "post_process"
        topic: /pipeline/post_processing_status
    }
}

// Standalone ONNX model for classification
onnx_model classification_model {
    config {
        input: "input" -> "images"
        output: "output" -> "classifications"
        device: "cpu"
        optimization: "openvino"
    }
}

// Standalone CUDA kernels block
cuda_kernels {
    kernel neural_network_kernel {
        block_size: (256, 1, 1)
        grid_size: (batch_size, 1, 1)
        shared_memory: 4096
        use_thrust: true
        input: float* input_data, float* weights, int input_size
        output: float* output_data, int output_size
        code: {
            __global__ void neural_forward(float* input, float* weights, float* output, 
                                         int input_sz, int output_sz) {
                int tid = blockIdx.x * blockDim.x + threadIdx.x;
                
                if (tid < output_sz) {
                    float sum = 0.0f;
                    for (int i = 0; i < input_sz; i++) {
                        sum += input[i] * weights[tid * input_sz + i];
                    }
                    output[tid] = tanhf(sum); // Activation function
                }
            }
        }
    }
}

// Simple node with basic features
node simple_publisher {
    parameter string message = "Hello, RoboDSL!"
    parameter int publish_rate = 1
    
    timer publish_timer: 1000 / publish_rate {
        oneshot: false
        autostart: true
    }
    
    publisher /simple/message: "std_msgs/String"
    
    method publish_message {
        code: {
            std_msgs::String msg;
            msg.data = message;
            publisher.publish(msg);
        }
    }
}

// Node with complex data structures
node data_processor {
    parameter float threshold = 0.5
    parameter int max_iterations = 100
    parameter string[] algorithms = ["algorithm1", "algorithm2"]
    parameter int[] nested_level2 = [1, 2, 3, 4, 5]
    
    parameter float[3][3] transformation_matrix = [
        [1.0, 0.0, 0.0],
        [0.0, 1.0, 0.0],
        [0.0, 0.0, 1.0]
    ]
    
    subscriber /data/input: "std_msgs/Float64MultiArray"
    publisher /data/output: "std_msgs/Float64MultiArray"
    
    method process_data {
        input: std::vector<double>* input_data
        output: std::vector<double>* output_data
        code: {
            // Process data using the transformation matrix
            for (size_t i = 0; i < input_data->size(); i += 3) {
                if (i + 2 < input_data->size()) {
                    double x = (*input_data)[i];
                    double y = (*input_data)[i + 1];
                    double z = (*input_data)[i + 2];
                    
                    // Apply transformation
                    double new_x = transformation_matrix[0][0] * x + 
                                  transformation_matrix[0][1] * y + 
                                  transformation_matrix[0][2] * z;
                    double new_y = transformation_matrix[1][0] * x + 
                                  transformation_matrix[1][1] * y + 
                                  transformation_matrix[1][2] * z;
                    double new_z = transformation_matrix[2][0] * x + 
                                  transformation_matrix[2][1] * y + 
                                  transformation_matrix[2][2] * z;
                    
                    output_data->push_back(new_x);
                    output_data->push_back(new_y);
                    output_data->push_back(new_z);
                }
            }
        }
    }
}

// Test node that uses a global CUDA kernel
node test_kernel_user {
    use_kernel: "neural_network_kernel"
    parameter int input_size = 128
    publisher /test/output: "std_msgs/Float32MultiArray"
    // This node uses the global kernel defined below
} 