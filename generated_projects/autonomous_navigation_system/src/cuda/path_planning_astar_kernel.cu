// Generated by RoboDSL - DO NOT EDIT


#include "cuda/path_planning_astar_kernel.cuh"
#include <algorithm>
#include <stdexcept>

// Define uchar type if not already defined
#ifndef __UCHAR_TYPE__
typedef unsigned char uchar;
#endif

namespace robodsl {

namespace {

// Device kernel implementation
__global__ void path_planning_astar_kernel(
    float** occupancy_grid,    int* grid_width,    int* grid_height,    int* start_x,    int* start_y,    int* goal_x,    int* goal_y,    int** path_x,    int** path_y,    int** path_length, int num_elements
) {
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    const int stride = blockDim.x * gridDim.x;

    // Process elements in parallel
    for (int i = idx; i < num_elements; i += stride) {
        // Kernel implementation here
        // Example: output[i] = process(input[i], parameters);
    }
}

} // anonymous namespace

path_planning_astarKernel::path_planning_astarKernel(cudaStream_t* stream)
    : stream_(stream) {
    // Initialize CUDA device if needed
    cudaError_t status = cudaGetDevice(&device_id_);
    if (status != cudaSuccess) {
        last_error_ = std::string("Failed to get CUDA device: ") + cudaGetErrorString(status);
        return;
    }
    
    // Get device properties
    cudaDeviceProp props;
    status = cudaGetDeviceProperties(&props, device_id_);
    if (status != cudaSuccess) {
        last_error_ = std::string("Failed to get device properties: ") + cudaGetErrorString(status);
        return;
    }
    
    // Calculate optimal block size based on device properties
    max_threads_per_block_ = props.maxThreadsPerBlock;
    max_blocks_per_dim_ = props.maxGridSize[0];
    
    // CUDA device initialized successfully
    // Device: %s (Compute %d.%d)
}

path_planning_astarKernel::~path_planning_astarKernel() {
    try {
        freeDeviceMemory();
    } catch (const std::exception& e) {
        // Error in destructor: %s
    }
}

bool path_planning_astarKernel::initialize(const float** input, size_t input_size) {
    if (!input || input_size == 0) {
        last_error_ = "Input pointer is null or size is zero";
        return false;
    }
    
    // Free any previously allocated memory
    freeDeviceMemory();
    
    // Store input size
    input_size_ = input_size;
    
    // Allocate device memory
    if (!allocateDeviceMemory()) {
        return false;
    }
    
    // Declare status variable once
    cudaError_t status;
    
    // Copy input data to device
    status = cudaMemcpyAsync(
        d_occupancy_grid_, 
        input, 
        input_size_ * sizeof(float*), 
        cudaMemcpyHostToDevice,
        stream_ ? *stream_ : 0
    );
    
    if (!checkCudaError(status, "Copy occupancy_grid to device")) {
        return false;
    }
    status = cudaMemcpyAsync(
        d_grid_width_, 
        input, 
        input_size_ * sizeof(int), 
        cudaMemcpyHostToDevice,
        stream_ ? *stream_ : 0
    );
    
    if (!checkCudaError(status, "Copy grid_width to device")) {
        return false;
    }
    status = cudaMemcpyAsync(
        d_grid_height_, 
        input, 
        input_size_ * sizeof(int), 
        cudaMemcpyHostToDevice,
        stream_ ? *stream_ : 0
    );
    
    if (!checkCudaError(status, "Copy grid_height to device")) {
        return false;
    }
    status = cudaMemcpyAsync(
        d_start_x_, 
        input, 
        input_size_ * sizeof(int), 
        cudaMemcpyHostToDevice,
        stream_ ? *stream_ : 0
    );
    
    if (!checkCudaError(status, "Copy start_x to device")) {
        return false;
    }
    status = cudaMemcpyAsync(
        d_start_y_, 
        input, 
        input_size_ * sizeof(int), 
        cudaMemcpyHostToDevice,
        stream_ ? *stream_ : 0
    );
    
    if (!checkCudaError(status, "Copy start_y to device")) {
        return false;
    }
    status = cudaMemcpyAsync(
        d_goal_x_, 
        input, 
        input_size_ * sizeof(int), 
        cudaMemcpyHostToDevice,
        stream_ ? *stream_ : 0
    );
    
    if (!checkCudaError(status, "Copy goal_x to device")) {
        return false;
    }
    status = cudaMemcpyAsync(
        d_goal_y_, 
        input, 
        input_size_ * sizeof(int), 
        cudaMemcpyHostToDevice,
        stream_ ? *stream_ : 0
    );
    
    if (!checkCudaError(status, "Copy goal_y to device")) {
        return false;
    }
    
    // Synchronize to ensure the copy is complete
    if (stream_) {
        status = cudaStreamSynchronize(*stream_);
    } else {
        status = cudaDeviceSynchronize();
    }
    
    if (!checkCudaError(status, "Synchronize after initialization")) {
        return false;
    }
    
    initialized_ = true;
    return true;
}

bool path_planning_astarKernel::process(const int** parameters, size_t param_size, int** output, size_t output_size) {
    if (!initialized_) {
        return false;
    }
    
    if (!output || output_size < input_size_) {
        last_error_ = "Output buffer is null or too small";
        return false;
    }
    
    // Declare status variable for this method
    cudaError_t status;
    
    // Copy parameters to device if needed
    if (parameters && !parameters_copied_) {
        status = cudaMemcpyAsync(
            d_parameters_, 
            parameters, 
            param_size, 
            cudaMemcpyHostToDevice,
            stream_ ? *stream_ : 0
        );
        
        if (!checkCudaError(status, "Copy parameters to device")) {
            return false;
        }
        
        parameters_copied_ = true;
    }
    
    // Calculate grid and block dimensions
    int block_size = std::min(max_threads_per_block_, kBlockSize);
    int num_blocks = (input_size_ + block_size - 1) / block_size;
    num_blocks = std::min(num_blocks, max_blocks_per_dim_);
    
    // Launch kernel
    path_planning_astar_kernel<<<num_blocks, block_size, 0, stream_ ? *stream_ : 0>>>(
        d_occupancy_grid_,        d_grid_width_,        d_grid_height_,        d_start_x_,        d_start_y_,        d_goal_x_,        d_goal_y_,        d_path_x_,        d_path_y_,        d_path_length_, input_size_
    );
    
    // Check for kernel launch errors
    status = cudaGetLastError();
    if (!checkCudaError(status, "Kernel launch")) {
        return false;
    }
    
    // Copy results back to host
    status = cudaMemcpyAsync(
        output, 
        d_path_x_, 
        input_size_ * sizeof(int*), 
        cudaMemcpyDeviceToHost,
        stream_ ? *stream_ : 0
    );
    
    if (!checkCudaError(status, "Copy path_x from device")) {
        return false;
    }
    status = cudaMemcpyAsync(
        output, 
        d_path_y_, 
        input_size_ * sizeof(int*), 
        cudaMemcpyDeviceToHost,
        stream_ ? *stream_ : 0
    );
    
    if (!checkCudaError(status, "Copy path_y from device")) {
        return false;
    }
    status = cudaMemcpyAsync(
        output, 
        d_path_length_, 
        input_size_ * sizeof(int*), 
        cudaMemcpyDeviceToHost,
        stream_ ? *stream_ : 0
    );
    
    if (!checkCudaError(status, "Copy path_length from device")) {
        return false;
    }
    
    // Synchronize to ensure the copy is complete
    if (stream_) {
        status = cudaStreamSynchronize(*stream_);
    } else {
        status = cudaDeviceSynchronize();
    }
    
    if (!checkCudaError(status, "Synchronize after processing")) {
        return false;
    }
    
    return true;
}

bool path_planning_astarKernel::allocateDeviceMemory() {
    cudaError_t status;
    
    // Allocate memory for all parameters
    status = cudaMalloc(&d_occupancy_grid_, input_size_ * sizeof(float*));
    if (!checkCudaError(status, "Allocate occupancy_grid memory")) {
        // Free previously allocated memory
        return false;
    }
    status = cudaMalloc(&d_grid_width_, input_size_ * sizeof(int));
    if (!checkCudaError(status, "Allocate grid_width memory")) {
        // Free previously allocated memory
        if (d_occupancy_grid_) {
            cudaFree(d_occupancy_grid_);
            d_occupancy_grid_ = nullptr;
        }
        return false;
    }
    status = cudaMalloc(&d_grid_height_, input_size_ * sizeof(int));
    if (!checkCudaError(status, "Allocate grid_height memory")) {
        // Free previously allocated memory
        if (d_occupancy_grid_) {
            cudaFree(d_occupancy_grid_);
            d_occupancy_grid_ = nullptr;
        }
        if (d_grid_width_) {
            cudaFree(d_grid_width_);
            d_grid_width_ = nullptr;
        }
        return false;
    }
    status = cudaMalloc(&d_start_x_, input_size_ * sizeof(int));
    if (!checkCudaError(status, "Allocate start_x memory")) {
        // Free previously allocated memory
        if (d_occupancy_grid_) {
            cudaFree(d_occupancy_grid_);
            d_occupancy_grid_ = nullptr;
        }
        if (d_grid_width_) {
            cudaFree(d_grid_width_);
            d_grid_width_ = nullptr;
        }
        if (d_grid_height_) {
            cudaFree(d_grid_height_);
            d_grid_height_ = nullptr;
        }
        return false;
    }
    status = cudaMalloc(&d_start_y_, input_size_ * sizeof(int));
    if (!checkCudaError(status, "Allocate start_y memory")) {
        // Free previously allocated memory
        if (d_occupancy_grid_) {
            cudaFree(d_occupancy_grid_);
            d_occupancy_grid_ = nullptr;
        }
        if (d_grid_width_) {
            cudaFree(d_grid_width_);
            d_grid_width_ = nullptr;
        }
        if (d_grid_height_) {
            cudaFree(d_grid_height_);
            d_grid_height_ = nullptr;
        }
        if (d_start_x_) {
            cudaFree(d_start_x_);
            d_start_x_ = nullptr;
        }
        return false;
    }
    status = cudaMalloc(&d_goal_x_, input_size_ * sizeof(int));
    if (!checkCudaError(status, "Allocate goal_x memory")) {
        // Free previously allocated memory
        if (d_occupancy_grid_) {
            cudaFree(d_occupancy_grid_);
            d_occupancy_grid_ = nullptr;
        }
        if (d_grid_width_) {
            cudaFree(d_grid_width_);
            d_grid_width_ = nullptr;
        }
        if (d_grid_height_) {
            cudaFree(d_grid_height_);
            d_grid_height_ = nullptr;
        }
        if (d_start_x_) {
            cudaFree(d_start_x_);
            d_start_x_ = nullptr;
        }
        if (d_start_y_) {
            cudaFree(d_start_y_);
            d_start_y_ = nullptr;
        }
        return false;
    }
    status = cudaMalloc(&d_goal_y_, input_size_ * sizeof(int));
    if (!checkCudaError(status, "Allocate goal_y memory")) {
        // Free previously allocated memory
        if (d_occupancy_grid_) {
            cudaFree(d_occupancy_grid_);
            d_occupancy_grid_ = nullptr;
        }
        if (d_grid_width_) {
            cudaFree(d_grid_width_);
            d_grid_width_ = nullptr;
        }
        if (d_grid_height_) {
            cudaFree(d_grid_height_);
            d_grid_height_ = nullptr;
        }
        if (d_start_x_) {
            cudaFree(d_start_x_);
            d_start_x_ = nullptr;
        }
        if (d_start_y_) {
            cudaFree(d_start_y_);
            d_start_y_ = nullptr;
        }
        if (d_goal_x_) {
            cudaFree(d_goal_x_);
            d_goal_x_ = nullptr;
        }
        return false;
    }
    status = cudaMalloc(&d_path_x_, input_size_ * sizeof(int*));
    if (!checkCudaError(status, "Allocate path_x memory")) {
        // Free previously allocated memory
        if (d_occupancy_grid_) {
            cudaFree(d_occupancy_grid_);
            d_occupancy_grid_ = nullptr;
        }
        if (d_grid_width_) {
            cudaFree(d_grid_width_);
            d_grid_width_ = nullptr;
        }
        if (d_grid_height_) {
            cudaFree(d_grid_height_);
            d_grid_height_ = nullptr;
        }
        if (d_start_x_) {
            cudaFree(d_start_x_);
            d_start_x_ = nullptr;
        }
        if (d_start_y_) {
            cudaFree(d_start_y_);
            d_start_y_ = nullptr;
        }
        if (d_goal_x_) {
            cudaFree(d_goal_x_);
            d_goal_x_ = nullptr;
        }
        if (d_goal_y_) {
            cudaFree(d_goal_y_);
            d_goal_y_ = nullptr;
        }
        return false;
    }
    status = cudaMalloc(&d_path_y_, input_size_ * sizeof(int*));
    if (!checkCudaError(status, "Allocate path_y memory")) {
        // Free previously allocated memory
        if (d_occupancy_grid_) {
            cudaFree(d_occupancy_grid_);
            d_occupancy_grid_ = nullptr;
        }
        if (d_grid_width_) {
            cudaFree(d_grid_width_);
            d_grid_width_ = nullptr;
        }
        if (d_grid_height_) {
            cudaFree(d_grid_height_);
            d_grid_height_ = nullptr;
        }
        if (d_start_x_) {
            cudaFree(d_start_x_);
            d_start_x_ = nullptr;
        }
        if (d_start_y_) {
            cudaFree(d_start_y_);
            d_start_y_ = nullptr;
        }
        if (d_goal_x_) {
            cudaFree(d_goal_x_);
            d_goal_x_ = nullptr;
        }
        if (d_goal_y_) {
            cudaFree(d_goal_y_);
            d_goal_y_ = nullptr;
        }
        if (d_path_x_) {
            cudaFree(d_path_x_);
            d_path_x_ = nullptr;
        }
        return false;
    }
    status = cudaMalloc(&d_path_length_, input_size_ * sizeof(int*));
    if (!checkCudaError(status, "Allocate path_length memory")) {
        // Free previously allocated memory
        if (d_occupancy_grid_) {
            cudaFree(d_occupancy_grid_);
            d_occupancy_grid_ = nullptr;
        }
        if (d_grid_width_) {
            cudaFree(d_grid_width_);
            d_grid_width_ = nullptr;
        }
        if (d_grid_height_) {
            cudaFree(d_grid_height_);
            d_grid_height_ = nullptr;
        }
        if (d_start_x_) {
            cudaFree(d_start_x_);
            d_start_x_ = nullptr;
        }
        if (d_start_y_) {
            cudaFree(d_start_y_);
            d_start_y_ = nullptr;
        }
        if (d_goal_x_) {
            cudaFree(d_goal_x_);
            d_goal_x_ = nullptr;
        }
        if (d_goal_y_) {
            cudaFree(d_goal_y_);
            d_goal_y_ = nullptr;
        }
        if (d_path_x_) {
            cudaFree(d_path_x_);
            d_path_x_ = nullptr;
        }
        if (d_path_y_) {
            cudaFree(d_path_y_);
            d_path_y_ = nullptr;
        }
        return false;
    }
    
    return true;
}

void path_planning_astarKernel::freeDeviceMemory() {
    if (d_occupancy_grid_) {
        cudaFree(d_occupancy_grid_);
        d_occupancy_grid_ = nullptr;
    }
    if (d_grid_width_) {
        cudaFree(d_grid_width_);
        d_grid_width_ = nullptr;
    }
    if (d_grid_height_) {
        cudaFree(d_grid_height_);
        d_grid_height_ = nullptr;
    }
    if (d_start_x_) {
        cudaFree(d_start_x_);
        d_start_x_ = nullptr;
    }
    if (d_start_y_) {
        cudaFree(d_start_y_);
        d_start_y_ = nullptr;
    }
    if (d_goal_x_) {
        cudaFree(d_goal_x_);
        d_goal_x_ = nullptr;
    }
    if (d_goal_y_) {
        cudaFree(d_goal_y_);
        d_goal_y_ = nullptr;
    }
    if (d_path_x_) {
        cudaFree(d_path_x_);
        d_path_x_ = nullptr;
    }
    if (d_path_y_) {
        cudaFree(d_path_y_);
        d_path_y_ = nullptr;
    }
    if (d_path_length_) {
        cudaFree(d_path_length_);
        d_path_length_ = nullptr;
    }
    
    input_size_ = 0;
    initialized_ = false;
    parameters_copied_ = false;
}

bool path_planning_astarKernel::checkCudaError(cudaError_t status, const std::string& context) {
    if (status != cudaSuccess) {
        last_error_ = context + ": " + cudaGetErrorString(status);
        return false;
    }
    return true;
}

} // namespace robodsl
