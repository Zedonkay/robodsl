<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CUDA Acceleration | RoboDSL</title>
    <link rel="stylesheet" href="css/style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/atom-one-dark.min.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>RoboDSL</h1>
            <p>Domain-Specific Language for GPU-Accelerated Robotics</p>
        </header>
        
        <nav id="sidebar">
            <div class="search-box">
                <input type="text" id="search" placeholder="Search...">
                <button><i class="fas fa-search"></i></button>
            </div>
            <ul class="menu">
                <li><a href="index.html"><i class="fas fa-home"></i> Home</a></li>
                <li><a href="getting-started.html"><i class="fas fa-rocket"></i> Getting Started</a></li>
                <li class="has-submenu">
                    <a href="#"><i class="fas fa-book"></i> Documentation <i class="fas fa-chevron-down"></i></a>
                    <ul class="submenu">
                        <li><a href="developer-guide.html">Developer Guide</a></li>
                        <li><a href="dsl-specification.html">DSL Specification</a></li>
                        <li><a href="cuda-acceleration.html" class="active">CUDA Acceleration</a></li>
                        <li><a href="faq.html">FAQ</a></li>
                    </ul>
                </li>
                <li class="has-submenu">
                    <a href="#"><i class="fas fa-star"></i> Features <i class="fas fa-chevron-down"></i></a>
                    <ul class="submenu">
                        <li><a href="dsl-specification.html#lifecycle-nodes">ROS2 Lifecycle</a></li>
                        <li><a href="dsl-specification.html#qos-configuration">QoS Configuration</a></li>
                        <li><a href="cuda-acceleration.html">CUDA Integration</a></li>
                    </ul>
                </li>
                <li class="has-submenu">
                    <a href="#"><i class="fas fa-users"></i> Community <i class="fas fa-chevron-down"></i></a>
                    <ul class="submenu">
                        <li><a href="contributing.html">Contributing</a></li>
                        <li><a href="code-of-conduct.html">Code of Conduct</a></li>
                    </ul>
                </li>
            </ul>
        </nav>

        <main id="content">
            <nav class="breadcrumb mb-6">
                <a href="index.html">Home</a> &gt; <a href="#">Documentation</a> &gt; CUDA Acceleration
            </nav>

            <article class="prose max-w-none">
                <h1>CUDA Acceleration</h1>
                <p class="lead">
                    RoboDSL provides first-class support for CUDA acceleration, allowing you to write high-performance 
                    GPU-accelerated code directly in your RoboDSL files.
                </p>

                <div class="toc">
                    <h2>Table of Contents</h2>
                    <ul>
                        <li><a href="#defining-cuda-kernels">Defining CUDA Kernels</a></li>
                        <li><a href="#memory-management">Memory Management</a></li>
                        <li><a href="#streams-and-events">Streams and Events</a></li>
                        <li><a href="#multi-gpu-support">Multi-GPU Support</a></li>
                        <li><a href="#thrust-integration">Thrust Integration</a></li>
                        <li><a href="#performance-optimization">Performance Optimization</a></li>
                        <li><a href="#debugging-cuda-code">Debugging CUDA Code</a></li>
                    </ul>
                </div>

                <section id="defining-cuda-kernels">
                    <h2>Defining CUDA Kernels</h2>
                    
                    <h3>Basic Kernel Definition</h3>
                    <pre><code class="language-robodsl">// Define a CUDA kernel
cuda_kernels {
    kernel vector_add {
        // Input parameters
        input float* a
        input float* b
        output float* c
        input int n
        
        // Kernel configuration
        block_size = (256, 1, 1)
        grid_size = ((n + 255) / 256, 1, 1)
        
        // Kernel code
        code {
            __global__ void vector_add_kernel(
                const float* a,
                const float* b,
                float* c,
                int n) {
                
                int i = blockIdx.x * blockDim.x + threadIdx.x;
                if (i < n) {
                    c[i] = a[i] + b[i];
                }
            }
        }
    }
}</code></pre>

                    <h3>Kernel with Shared Memory</h3>
                    <pre><code class="language-robodsl">cuda_kernels {
    kernel matrix_multiply {
        // Input parameters
        input float* a
        input float* b
        output float* c
        input int width
        
        // Shared memory declaration
        shared: "extern __shared__ float sdata[];"
        
        // Kernel configuration
        block_size = (16, 16, 1)
        grid_size = (width / 16, width / 16, 1)
        
        // Shared memory size (in bytes)
        shared_mem_size: "2 * 16 * 16 * sizeof(float)"
        
        // Kernel code
        code {
            __global__ void matrix_multiply_kernel(
                const float* A,
                const float* B,
                float* C,
                int width) {
                
                // 2D thread and block indices
                int tx = threadIdx.x;
                int ty = threadIdx.y;
                int bx = blockIdx.x;
                int by = blockIdx.y;
                
                // Shared memory for tiles
                extern __shared__ float sdata[];
                float* sA = sdata;
                float* sB = sdata + 16 * 16;
                
                // Calculate row and column for this thread
                int row = by * blockDim.y + ty;
                int col = bx * blockDim.x + tx;
                
                float sum = 0.0f;
                
                // Loop over tiles
                for (int t = 0; t < width / 16; ++t) {
                    // Load tiles into shared memory
                    sA[ty * 16 + tx] = A[row * width + (t * 16 + tx)];
                    sB[ty * 16 + tx] = B[(t * 16 + ty) * width + col];
                    
                    // Synchronize to ensure all threads have loaded their data
                    __syncthreads();
                    
                    // Compute partial product
                    for (int k = 0; k < 16; ++k) {
                        sum += sA[ty * 16 + k] * sB[k * 16 + tx];
                    }
                    
                    // Synchronize before next tile
                    __syncthreads();
                }
                
                // Write result to global memory
                if (row < width && col < width) {
                    C[row * width + col] = sum;
                }
            }
        }
    }
}</code></pre>
                </section>

                <section id="memory-management">
                    <h2>Memory Management</h2>
                    
                    <h3>Device Memory Allocation</h3>
                    <p>RoboDSL provides automatic memory management for device memory:</p>
                    
                    <pre><code class="language-robodsl">node memory_example {
    // Allocate device memory
    device_memory {
        // Allocate a float array on the device
        float* d_a = cuda_malloc<float>(1024)
        
        // Allocate a 2D array
        float* d_matrix = cuda_malloc_2d<float>(1024, 1024)
        
        // Allocate a 3D array
        float* d_volume = cuda_malloc_3d<float>(256, 256, 256)
        
        // Memory is automatically freed when it goes out of scope
    }
    
    // Or use smart pointers for automatic management
    device_ptr<float> d_data = cuda_make_shared<float>(1024)
    
    // Access the raw pointer when needed
    float* raw_ptr = d_data.get()
}</code></pre>

                    <h3>Memory Transfers</h3>
                    <pre><code class="language-robodsl">node memory_transfer_example {
    // Host data
    std::vector<float> h_data(1024, 1.0f)
    
    // Allocate device memory
    device_ptr<float> d_data = cuda_make_shared<float>(1024)
    
    // Copy host to device
    cuda_memcpy_host_to_device(d_data.get(), h_data.data(), 1024 * sizeof(float))
    
    // Process data on device...
    
    // Copy device to host
    std::vector<float> h_result(1024)
    cuda_memcpy_device_to_host(h_result.data(), d_data.get(), 1024 * sizeof(float))
    
    // Asynchronous copy with stream
    cuda_stream stream = cuda_stream_create()
    cuda_memcpy_async(
        d_data.get(), 
        h_data.data(), 
        1024 * sizeof(float), 
        cudaMemcpyHostToDevice, 
        stream
    )
    
    // Wait for the copy to complete
    cuda_stream_synchronize(stream)
    cuda_stream_destroy(stream)
}</code></pre>
                </section>

                <section id="streams-and-events">
                    <h2>Streams and Events</h2>
                    <h3>Using Streams for Concurrent Execution</h3>
                    <pre><code class="language-robodsl">node stream_example {
    // Create streams
    cuda_stream stream1 = cuda_stream_create()
    cuda_stream stream2 = cuda_stream_create()
    
    // Allocate host and device memory
    std::vector<float> h_data1(1024, 1.0f)
    std::vector<float> h_data2(1024, 2.0f)
    device_ptr<float> d_data1 = cuda_make_shared<float>(1024)
    device_ptr<float> d_data2 = cuda_make_shared<float>(1024)
    
    // Asynchronous memory copies in different streams
    cuda_memcpy_async(
        d_data1.get(), 
        h_data1.data(), 
        1024 * sizeof(float), 
        cudaMemcpyHostToDevice, 
        stream1
    )
    
    cuda_memcpy_async(
        d_data2.get(), 
        h_data2.data(), 
        1024 * sizeof(float), 
        cudaMemcpyHostToDevice, 
        stream2
    )
    
    // Process data in parallel...
    
    // Synchronize all streams
    cuda_stream_synchronize(stream1)
    cuda_stream_synchronize(stream2)
    
    // Clean up
    cuda_stream_destroy(stream1)
    cuda_stream_destroy(stream2)
}</code></pre>
                </section>

                <section id="performance-optimization">
                    <h2>Performance Optimization</h2>
                    <p>Here are some tips for optimizing CUDA code in RoboDSL:</p>
                    <ul>
                        <li>Use shared memory for frequently accessed data</li>
                        <li>Ensure memory coalescing for global memory access</li>
                        <li>Minimize thread divergence within warps</li>
                        <li>Use asynchronous memory transfers with streams</li>
                        <li>Profile your kernels with NVIDIA Nsight Systems</li>
                    </ul>
                </section>

                <section id="debugging-cuda-code">
                    <h2>Debugging CUDA Code</h2>
                    <p>RoboDSL integrates with standard CUDA debugging tools:</p>
                    <ul>
                        <li>Use <code>cudaGetLastError()</code> to check for errors</li>
                        <li>Compile with <code>--device-debug</code> for device-side debugging</li>
                        <li>Use <code>printf</code> in device code (requires Compute Capability 2.0+)</li>
                        <li>Use NVIDIA Nsight for advanced debugging and profiling</li>
                    </ul>
                </section>
            </article>
        </main>

        <footer>
            <p>&copy; 2023 RoboDSL. All rights reserved.</p>
        </footer>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
    <script>
        // Initialize syntax highlighting
        document.addEventListener('DOMContentLoaded', (event) => {
            document.querySelectorAll('pre code').forEach((el) => {
                hljs.highlightElement(el);
            });
        });

        // Make table of contents links work
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                document.querySelector(this.getAttribute('href')).scrollIntoView({
                    behavior: 'smooth'
                });
            });
        });
    </script>
</body>
</html>
