// Generated by RoboDSL - DO NOT EDIT

{% for include in cuda_includes %}
#include <{{ include }}>
{% endfor %}

#ifndef {{ include_guard }}
#define {{ include_guard }}

{% if cuda_enabled %}


#include <cuda_runtime.h>
#include <cstdint>
#include <string>

// Define uchar type if not already defined
#ifndef __UCHAR_TYPE__
typedef unsigned char uchar;
#endif



namespace {{ namespace }} {

{% if struct_name and parameters %}
// Struct for kernel parameters
struct {{ struct_name }} {
{% for param in parameters %}
    {{ param.type }} {{ param.name }};
{% endfor %}
};
{% endif %}

/**
 * @brief CUDA kernel wrapper class for {{ kernel_name }}
 * 
 * This class provides a C++ interface to CUDA kernels, handling memory management
 * and kernel launches. It's designed to work with the ROS2 node lifecycle.
 */
class {{ kernel_name }}Kernel {
public:
    /**
     * @brief Construct a new {{ kernel_name }} Kernel object
     * 
     * @param stream CUDA stream to use for kernel execution (nullptr for default stream)
     */
    explicit {{ kernel_name }}Kernel(cudaStream_t* stream = nullptr);
    
    /**
     * @brief Destroy the {{ kernel_name }} Kernel object
     * 
     * Frees all allocated device memory.
     */
    ~{{ kernel_name }}Kernel();
    
    // Delete copy constructor and assignment operator
    {{ kernel_name }}Kernel(const {{ kernel_name }}Kernel&) = delete;
    {{ kernel_name }}Kernel& operator=(const {{ kernel_name }}Kernel&) = delete;
    
    /**
     * @brief Initialize the kernel with input data
     * 
     * @param input Input data to process
     * @return true if initialization was successful, false otherwise
     */
    // Initialize the kernel with input data (host pointer and size)
    bool initialize(const {{ input_type }}* input, size_t input_size);
    
    /**
     * @brief Process the input data using the CUDA kernel
     * 
     * @param parameters Kernel parameters
     * @return std::vector<{{ output_type }}> Processed output data
     */
    // Process the input data using the CUDA kernel (host pointer and size)
    bool process(const {{ param_type }}* parameters, size_t param_size, {{ output_type }}* output, size_t output_size);
    
    /**
     * @brief Get the last error message, if any
     * 
     * @return std::string The last error message, or an empty string if no error
     */
    const std::string& getLastError() const { return last_error_; }
    
    /**
     * @brief Check if the kernel was initialized successfully
     * 
     * @return true if initialized, false otherwise
     */
    bool isInitialized() const { return initialized_; }
    
private:
    // Device memory pointers
    {% for member in members %}
    {{ member.type }}* {{ member.name }} = nullptr;  //!< Device memory for {{ member.original_name }}
    {% endfor %}

    // Kernel state and configuration
    size_t input_size_ = 0;                //!< Number of input elements
    int device_id_ = 0;                    //!< CUDA device ID
    int max_threads_per_block_ = 256;      //!< Max threads per block
    int max_blocks_per_dim_ = 65535;       //!< Max blocks per grid dimension


    // Parameter state (for kernels with parameters)
    bool parameters_copied_ = false;
    {% if param_type != "void" %}
    {{ param_type }} last_parameters_{};
    {{ param_type }}* d_parameters_ = nullptr;
    {% endif %}

    cudaStream_t* stream_ = nullptr;  //!< CUDA stream for async operations
    bool initialized_ = false;        //!< Whether the kernel is properly initialized
    std::string last_error_;         //!< Last error message, if any
    
    // CUDA kernel launch configuration
    static constexpr int kBlockSize = {{ block_size | default(256) }};  //!< Threads per block
    
    /**
     * @brief Check for CUDA errors and update last_error_ if needed
     * 
     * @param status CUDA status to check
     * @param context Context string for error messages
     * @return true if no error, false otherwise
     */
    bool checkCudaError(cudaError_t status, const std::string& context);
    
    /**
     * @brief Allocate device memory
     * 
     * @return true if allocation was successful, false otherwise
     */
    bool allocateDeviceMemory();
    
    /**
     * @brief Free device memory
     */
    void freeDeviceMemory();
};

// Kernel function declaration (must be a free function, not a member function)
__global__ void {{ kernel_name }}_kernel(
    {%- for member in members %}
    const {{ member.type }}* {{ member.name }}{% if not loop.last or members|length > 0 %},{% endif %}
    {%- endfor %}
    int num_elements
);

} // namespace {{ namespace }}

{% else %}

// Dummy implementation when CUDA is not enabled
#include <vector>

namespace {{ namespace }} {

class {{ kernel_name }}Kernel {
public:
    explicit {{ kernel_name }}Kernel(void* /*stream*/ = nullptr) {}
    ~{{ kernel_name }}Kernel() = default;
    
    bool initialize(const std::vector<{{ input_type }}>& /*input*/) { 
        return false; 
    }
    
    std::vector<{{ output_type }}> process(const {{ param_type }}& /*parameters*/) {
        return {};
    }
    
    std::string getLastError() const { 
        return "CUDA support not compiled in"; 
    }
    
    bool isInitialized() const { return false; }
};

} // namespace {{ namespace }}

{% endif %}

#endif // {{ include_guard }}
