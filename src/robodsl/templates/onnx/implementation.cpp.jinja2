#include "{{ node_name }}_onnx.hpp"
#include <iostream>
#include <fstream>
#include <stdexcept>
#include <algorithm>
#include <cuda_runtime.h>

{{ node_name }}OnnxInference::{{ node_name }}OnnxInference(const std::string& model_path)
    : model_path_(model_path), device_type_("{{ device_type }}") {
    
    // Set up environment
    env_ = Ort::Env(ORT_LOGGING_LEVEL_WARNING, "{{ node_name }}_onnx");
}

{{ node_name }}OnnxInference::~{{ node_name }}OnnxInference() {
    // Free CUDA memory
    for (void* ptr : cuda_memory_pool_) {
        if (ptr) {
            cudaFree(ptr);
        }
    }
    cuda_memory_pool_.clear();
    
    // ONNX Runtime handles cleanup automatically
}

bool {{ node_name }}OnnxInference::initialize() {
    try {
        // Check if model file exists
        std::ifstream file_check(model_path_);
        if (!file_check.good()) {
            std::cerr << "Error: ONNX model file not found: " << model_path_ << std::endl;
            return false;
        }
        
        // Initialize CUDA if needed
        if (device_type_ == "cuda") {
            cudaError_t cuda_error = cudaSetDevice(0);
            if (cuda_error != cudaSuccess) {
                std::cerr << "Error: Failed to set CUDA device: " << cudaGetErrorString(cuda_error) << std::endl;
                return false;
            }
        }
        
        // Create session options
        Ort::SessionOptions session_options;
        initialize_session_options(session_options);
        
        // Create session
        session_ = Ort::Session(env_, model_path_.c_str(), session_options);
        
        // Get input/output information
        Ort::AllocatorWithDefaultOptions allocator;
        
        // Get input info
        input_name_ = session_.GetInputName(0, allocator);
        input_shape_ = session_.GetInputTypeInfo(0).GetTensorTypeAndShapeInfo().GetShape();
        
        // Get output info
        output_name_ = session_.GetOutputName(0, allocator);
        output_shape_ = session_.GetOutputTypeInfo(0).GetTensorTypeAndShapeInfo().GetShape();
        
        std::cout << "ONNX model initialized successfully:" << std::endl;
        std::cout << "  Model: " << model_path_ << std::endl;
        std::cout << "  Device: " << device_type_ << std::endl;
        std::cout << "  Input: " << input_name_ << " (shape: ";
        for (size_t i = 0; i < input_shape_.size(); ++i) {
            if (i > 0) std::cout << "x";
            std::cout << input_shape_[i];
        }
        std::cout << ")" << std::endl;
        std::cout << "  Output: " << output_name_ << " (shape: ";
        for (size_t i = 0; i < output_shape_.size(); ++i) {
            if (i > 0) std::cout << "x";
            std::cout << output_shape_[i];
        }
        std::cout << ")" << std::endl;
        
        return true;
        
    } catch (const Ort::Exception& e) {
        std::cerr << "ONNX Runtime error during initialization: " << e.what() << std::endl;
        return false;
    } catch (const std::exception& e) {
        std::cerr << "Error during ONNX model initialization: " << e.what() << std::endl;
        return false;
    }
}

bool {{ node_name }}OnnxInference::run_inference(const std::vector<float>& input_data, std::vector<float>& output_data) {
    try {
        // Preprocess input (pure C++)
        std::vector<float> processed_input = preprocess_input(input_data);
        
        // Create input tensor
        std::vector<const char*> input_names = {input_name_.c_str()};
        std::vector<const char*> output_names = {output_name_.c_str()};
        
        // Create input tensor
        auto input_tensor = Ort::Value::CreateTensor<float>(
            allocator_, 
            const_cast<float*>(processed_input.data()), 
            processed_input.size(),
            input_shape_.data(), 
            input_shape_.size()
        );
        
        // Run inference
        auto output_tensors = session_.Run(
            Ort::RunOptions{nullptr}, 
            input_names.data(), 
            &input_tensor, 
            1, 
            output_names.data(), 
            1
        );
        
        // Extract output data
        float* output_buffer = output_tensors[0].GetTensorMutableData<float>();
        size_t output_size = output_tensors[0].GetTensorTypeAndShapeInfo().GetElementCount();
        
        output_data.assign(output_buffer, output_buffer + output_size);
        
        // Postprocess output (pure C++)
        output_data = postprocess_output(output_data);
        
        return true;
        
    } catch (const Ort::Exception& e) {
        std::cerr << "ONNX Runtime error during inference: " << e.what() << std::endl;
        return false;
    } catch (const std::exception& e) {
        std::cerr << "Error during inference: " << e.what() << std::endl;
        return false;
    }
}

bool {{ node_name }}OnnxInference::run_inference_cuda(const float* input_data, size_t input_size, 
                                                     float* output_data, size_t output_size) {
    try {
        // Allocate CUDA memory
        void* cuda_input = allocate_cuda_memory(input_size * sizeof(float));
        void* cuda_output = allocate_cuda_memory(output_size * sizeof(float));
        
        if (!cuda_input || !cuda_output) {
            return false;
        }
        
        // Copy input to CUDA
        cudaError_t cuda_error = cudaMemcpy(cuda_input, input_data, input_size * sizeof(float), cudaMemcpyHostToDevice);
        if (cuda_error != cudaSuccess) {
            std::cerr << "Error copying input to CUDA: " << cudaGetErrorString(cuda_error) << std::endl;
            return false;
        }
        
        // Create input tensor with CUDA memory
        std::vector<const char*> input_names = {input_name_.c_str()};
        std::vector<const char*> output_names = {output_name_.c_str()};
        
        auto input_tensor = Ort::Value::CreateTensor<float>(
            allocator_, 
            static_cast<float*>(cuda_input), 
            input_size,
            input_shape_.data(), 
            input_shape_.size()
        );
        
        // Run inference
        auto output_tensors = session_.Run(
            Ort::RunOptions{nullptr}, 
            input_names.data(), 
            &input_tensor, 
            1, 
            output_names.data(), 
            1
        );
        
        // Copy output from CUDA
        cuda_error = cudaMemcpy(output_data, cuda_output, output_size * sizeof(float), cudaMemcpyDeviceToHost);
        if (cuda_error != cudaSuccess) {
            std::cerr << "Error copying output from CUDA: " << cudaGetErrorString(cuda_error) << std::endl;
            return false;
        }
        
        return true;
        
    } catch (const Ort::Exception& e) {
        std::cerr << "ONNX Runtime error during CUDA inference: " << e.what() << std::endl;
        return false;
    } catch (const std::exception& e) {
        std::cerr << "Error during CUDA inference: " << e.what() << std::endl;
        return false;
    }
}

void* {{ node_name }}OnnxInference::allocate_cuda_memory(size_t size) {
    void* ptr = nullptr;
    cudaError_t error = cudaMalloc(&ptr, size);
    if (error == cudaSuccess) {
        cuda_memory_pool_.push_back(ptr);
        return ptr;
    } else {
        std::cerr << "Error allocating CUDA memory: " << cudaGetErrorString(error) << std::endl;
        return nullptr;
    }
}

void {{ node_name }}OnnxInference::free_cuda_memory(void* ptr) {
    if (ptr) {
        cudaFree(ptr);
        auto it = std::find(cuda_memory_pool_.begin(), cuda_memory_pool_.end(), ptr);
        if (it != cuda_memory_pool_.end()) {
            cuda_memory_pool_.erase(it);
        }
    }
}

bool {{ node_name }}OnnxInference::copy_to_cuda(const std::vector<float>& host_data, void* cuda_ptr) {
    cudaError_t error = cudaMemcpy(cuda_ptr, host_data.data(), 
                                   host_data.size() * sizeof(float), cudaMemcpyHostToDevice);
    return error == cudaSuccess;
}

bool {{ node_name }}OnnxInference::copy_from_cuda(void* cuda_ptr, std::vector<float>& host_data) {
    cudaError_t error = cudaMemcpy(host_data.data(), cuda_ptr, 
                                   host_data.size() * sizeof(float), cudaMemcpyDeviceToHost);
    return error == cudaSuccess;
}

void {{ node_name }}OnnxInference::initialize_session_options(Ort::SessionOptions& session_options) {
    // Set device type
    if (device_type_ == "cuda") {
        session_options.SetExecutionMode(ExecutionMode::ORT_PARALLEL);
        session_options.AppendExecutionProvider_CUDA(OrtCUDAProviderOptions{});
    } else {
        session_options.SetExecutionMode(ExecutionMode::ORT_SEQUENTIAL);
    }
    
    // Set optimization level
    {%- if optimizations %}
    {%- for opt in optimizations %}
    {%- if opt.optimization == 'tensorrt' %}
    // Enable TensorRT optimization
    session_options.SetOptimizedModelFilePath("{{ node_name }}_optimized.onnx");
    {%- endif %}
    {%- endfor %}
    {%- endif %}
    
    // Set other options
    session_options.SetIntraOpNumThreads(1);
    session_options.SetInterOpNumThreads(1);
    session_options.SetGraphOptimizationLevel(GraphOptimizationLevel::ORT_ENABLE_ALL);
}

std::vector<float> {{ node_name }}OnnxInference::preprocess_input(const std::vector<float>& raw_input) {
    // Pure C++ preprocessing - no Python dependencies
    std::vector<float> processed = raw_input;
    
    // Normalize if needed (example: normalize to [0, 1] range)
    // for (auto& val : processed) {
    //     val = val / 255.0f;
    // }
    
    return processed;
}

std::vector<float> {{ node_name }}OnnxInference::postprocess_output(const std::vector<float>& raw_output) {
    // Pure C++ postprocessing - no Python dependencies
    std::vector<float> processed = raw_output;
    
    // Apply softmax if needed (example)
    // float max_val = *std::max_element(processed.begin(), processed.end());
    // float sum = 0.0f;
    // for (auto& val : processed) {
    //     val = std::exp(val - max_val);
    //     sum += val;
    // }
    // for (auto& val : processed) {
    //     val /= sum;
    // }
    
    return processed;
} 